{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12ce2d16fc274c4da771e26b26b3634d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91dc01914538488a88efb57789d5694c",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97b5498f15f64c2983c0f21780091d21",
            "value": 100
          }
        },
        "91dc01914538488a88efb57789d5694c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "97b5498f15f64c2983c0f21780091d21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "black",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gitloaderを使用したソースコード問い合わせチャットボット"
      ],
      "metadata": {
        "id": "ulRQ6MLpRL1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 環境構築"
      ],
      "metadata": {
        "id": "umvezGLzQ5-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.0.218\n",
        "!pip install openai==0.27.8\n",
        "!pip install chromadb==0.3.26\n",
        "!pip install tiktoken==0.4.0\n",
        "!pip install GitPython==3.1.31"
      ],
      "metadata": {
        "id": "YZwA51jTQNqv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc68b15b-8d1a-42fe-de7b-ad0426824765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.218\n",
            "  Downloading langchain-0.0.218-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.218) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.218) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.218) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.218) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.218)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk>=0.0.17 (from langchain==0.0.218)\n",
            "  Downloading langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.218) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.218) (1.23.5)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.218)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.218) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.218) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.218) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.218) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.218) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.218) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.218) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.218) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.218) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.218)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.218)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.218) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.218) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.218) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.218) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.218) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.218) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.218)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, langchainplus-sdk, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.14 langchain-0.0.218 langchainplus-sdk-0.0.20 marshmallow-3.20.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCzmvPurP9OP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.document_loaders import GitLoader\n",
        "from datetime import datetime\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "#TODO: APIキーの登録が必要\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 実装方法"
      ],
      "metadata": {
        "id": "tXfl6kiwQ9r8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 現在の日時を取得\n",
        "now = datetime.now()\n",
        "\n",
        "# ミリ秒までの文字列にフォーマット\n",
        "formatted_now = now.strftime('%Y-%m-%d%H:%M:%S.%f')[:-3]\n",
        "\n",
        "print(formatted_now)\n",
        "\n",
        "clone_url = \"https://github.com/hwchase17/langchain\"\n",
        "branch = \"master\"\n",
        "repo_path = f\"./tempo/{formatted_now}\"\n",
        "filter_ext = \".py\"\n",
        "\n",
        "if os.path.exists(repo_path):\n",
        "    clone_url = None\n"
      ],
      "metadata": {
        "id": "ij8e4GBunXuB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3565847-de6a-48fa-8712-73925c0aa7c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-2915:50:30.963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_now"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "xYMWF6spopvE",
        "outputId": "fdce4487-b8da-4274-c097-29ed128f31a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2023-09-2915:50:30.963'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = GitLoader(\n",
        "    clone_url=clone_url,\n",
        "    branch=branch,\n",
        "    repo_path=repo_path,\n",
        "    file_filter=lambda file_path: file_path.endswith(filter_ext),\n",
        ")"
      ],
      "metadata": {
        "id": "rt69pi_VzvXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "id": "oNEafq5jmxcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "34FZw9yjpz-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "all_splits = text_splitter.split_documents(data)\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "12ce2d16fc274c4da771e26b26b3634d",
            "91dc01914538488a88efb57789d5694c",
            "97b5498f15f64c2983c0f21780091d21"
          ]
        },
        "id": "TBgRO-WUPdKS",
        "outputId": "fd954bf0-a453-4e44-de93-d72f4302177d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12ce2d16fc274c4da771e26b26b3634d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 永続化させる and 結果を良い感じのUIにレンダリングしたい\n",
        "    # client_settings = chromadb.config.Settings(\n",
        "    #     chroma_db_impl=\"duckdb+parquet\",\n",
        "    #     persist_directory=DB_DIR,\n",
        "    #     anonymized_telemetry=False\n",
        "    # )"
      ],
      "metadata": {
        "id": "Bp_CVrDZt4kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0)\n",
        "memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",return_messages=True)"
      ],
      "metadata": {
        "id": "VAfbiVh9Po8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI()\n",
        "retriever = vectorstore.as_retriever()\n",
        "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)"
      ],
      "metadata": {
        "id": "GeC206FtPvBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa (\"LangChainについて詳細に説明してください？\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6RT76XEPpMm",
        "outputId": "2c38076c-88b1-4a6f-b450-2164813c6675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'LangChainについて詳細に説明してください？',\n",
              " 'chat_history': [SystemMessage(content='', additional_kwargs={})],\n",
              " 'answer': 'LangChainは、自然言語処理（NLP）タスクを行うためのPythonパッケージです。LangChainには、さまざまなタスクを実行するための複数のチェーンが含まれています。これらのチェーンは、テキストデータの前処理、特徴エンジニアリング、モデルのトレーニング、推論などのステップを組み合わせてタスクを実行します。\\n\\nLangChainのバージョン2.0.0は現在利用可能ですが、3.0.0で廃止予定となっています。最新バージョンのLangChainを使用することをおすすめします。'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa (\"さまざまなタスクとは具体的にはどのようなタスクですか？\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccKESrR3P59Q",
        "outputId": "d6e418f9-5e6c-43a4-e4b5-adaa6a224fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'さまざまなタスクとは具体的にはどのようなタスクですか？',\n",
              " 'chat_history': [SystemMessage(content='\\nThe human asked for a detailed explanation of LangChain. The AI explained that LangChain is a Python package for performing natural language processing tasks, containing multiple chains for performing tasks such as preprocessing text data, feature engineering, training models, and inference. Version 2.0.0 of LangChain is currently available, but version 3.0.0 is scheduled to be discontinued. The AI recommended using the latest version of LangChain.', additional_kwargs={})],\n",
              " 'answer': 'さまざまなタスクには、以下のようなものが含まれます。\\n\\n1. 簡単な質問への回答：例えば、天気予報や時間帯の確認などの基本的な情報を提供します。\\n\\n2. 情報の解説や説明：特定のトピックに関する詳細な情報や概念の説明を行います。\\n\\n3. 会話やディスカッション：特定のトピックについての意見交換や議論に参加します。\\n\\n4. ヘルプやアドバイスの提供：特定の問題や課題に対して解決策やアドバイスを提案します。\\n\\n5. タスクの自動化：特定の作業や手続きを自動化するための手助けをします。\\n\\nこれらは一部の例ですが、私は多くの異なるタスクに対応することができます。どのような質問やタスクがあるか具体的にお知りになりたいですか？'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa (\"調べたいことについて最新データをgoogle検索から取得して答えてくれるようにLangChainを実装したいです。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGUpO5W8PiqV",
        "outputId": "bd2ed341-41c7-434f-d04b-b2030feddf8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': '調べたいことについて最新データをgoogle検索から取得して答えてくれるようにLangChainを実装したいです。',\n",
              " 'chat_history': [SystemMessage(content='\\nThe human asked for a detailed explanation of LangChain. The AI explained that LangChain is a Python package for performing natural language processing tasks, containing multiple chains for performing tasks such as preprocessing text data, feature engineering, training models, and inference. Version 2.0.0 of LangChain is currently available, but version 3.0.0 is scheduled to be discontinued. The AI recommended using the latest version of LangChain and mentioned that it can perform various tasks such as providing basic information, explaining detailed information and concepts, participating in conversations and discussions, providing help and advice, and automating tasks.', additional_kwargs={})],\n",
              " 'answer': 'はい、LangChainを使用して最新のGoogle検索データを取得する方法があります。具体的には、以下の手順を実行することで実現できます。\\n\\n1. Google検索APIを使用して、指定したキーワードに関連する検索結果を取得します。\\n2. 取得した検索結果をLangChainの入力として使用します。\\n3. LangChainは、取得した検索結果を処理し、必要な情報を抽出します。\\n\\nただし、LangChainの具体的な実装方法はプロジェクトによって異なるため、詳細な手順を提供することはできません。プロジェクトの要件と目的に合わせて、適切なLangChainの実装を行ってください。'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa (\"serp apiを使った実装をpythonで書いてください\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnrdjEd7rWLR",
        "outputId": "c1016d9a-6fff-4695-a6e2-c36f39424da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'serp apiを使った実装をpythonで書いてください',\n",
              " 'chat_history': [SystemMessage(content=\"\\nThe human asked for a detailed explanation of LangChain. The AI explained that LangChain is a Python package for performing natural language processing tasks, containing multiple chains for performing tasks such as preprocessing text data, feature engineering, training models, and inference. Version 2.0.0 of LangChain is currently available, but version 3.0.0 is scheduled to be discontinued. The AI recommended using the latest version of LangChain and mentioned that it can perform various tasks such as providing basic information, explaining detailed information and concepts, participating in conversations and discussions, providing help and advice, and automating tasks. The AI also explained that LangChain can be used to retrieve the latest Google search data for the desired information, by using the Google search API to get search results related to the specified keyword, and then using the results as input for LangChain to process and extract the necessary information. However, the exact implementation of LangChain varies depending on the project, so the AI cannot provide detailed steps. The AI suggested to implement the appropriate LangChain according to the project's requirements and objectives.\", additional_kwargs={})],\n",
              " 'answer': 'LangChainの実装手順については、提供された情報では具体的な手順を特定することはできません。ただし、Chainインターフェースを使用してLangChainを実装する場合、以下の手順を一般的に考慮することができます：\\n\\n1. Chainインターフェースを実装するクラスを作成します。\\n2. 必要に応じて、Chainに状態を追加するためにMemoryを実装します。\\n3. Callbacksを使用して、Chainに追加の機能（例：ログ記録）を実行することができます。\\n4. 必要に応じて、他のコンポーネントやChainと組み合わせるための柔軟なChain APIを使用します。\\n\\n具体的な実装手順は、プロジェクトの要件や目標によって異なる場合があります。詳細な手順については、LangSmithプロジェクトのドキュメントやソースコードを参照することをおすすめします。'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa (\"document loaderにはどのようなものがありますか？\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXCBATAhs9-k",
        "outputId": "e1e5fd1c-a26d-4d33-b5c4-d89553652520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'document loaderにはどのようなものがありますか？',\n",
              " 'chat_history': [SystemMessage(content=\"\\nThe human asked for a detailed explanation of LangChain. The AI explained that LangChain is a Python package for performing natural language processing tasks, containing multiple chains for performing tasks such as preprocessing text data, feature engineering, training models, and inference. Version 2.0.0 of LangChain is currently available, but version 3.0.0 is scheduled to be discontinued. The AI recommended using the latest version of LangChain and mentioned that it can perform various tasks such as providing basic information, explaining detailed information and concepts, participating in conversations and discussions, providing help and advice, and automating tasks. The AI also explained that LangChain can be used to retrieve the latest Google search data for the desired information, by using the Google search API to get search results related to the specified keyword, and then using the results as input for LangChain to process and extract the necessary information. However, the exact implementation of LangChain varies depending on the project, so the AI cannot provide detailed steps. The AI suggested to implement the appropriate LangChain according to the project's requirements and objectives, and recommended referring to the LangSmith project's documentation and source code for more detailed steps. When asked about specific modules that could increase the productivity of software engineers, the AI was unable to\", additional_kwargs={})],\n",
              " 'answer': '次のモジュールがDocument Loaderに含まれています：\\n\\n- AssemblyAIAudioTranscriptLoader\\n- AsyncHtmlLoader\\n- AzureBlobStorageContainerLoader\\n- AzureBlobStorageFileLoader\\n- BSHTMLLoader\\n- BibtexLoader\\n- BigQueryLoader\\n- BiliBiliLoader\\n- BlackboardLoader\\n- Blob\\n- BlobLoader\\n- BlockchainDocumentLoader\\n- BraveSearchLoader\\n- BrowserlessLoader\\n- CSVLoader\\n- ChatGPTLoader\\n- CoNLLULoader\\n- CollegeConfidentialLoader\\n- ConcurrentLoader\\n- ConfluenceLoader\\n- ModernTreasuryLoader\\n- MongodbLoader\\n- NewsURLLoader\\n- NotebookLoader\\n- NotionDBLoader\\n- NotionDirectoryLoader\\n- OBSDirectoryLoader\\n- OBSFileLoader\\n- ObsidianLoader\\n- OneDriveFileLoader\\n- OneDriveLoader\\n- OnlinePDFLoader\\n- OpenCityDataLoader\\n- OutlookMessageLoader\\n- PDFMinerLoader\\n- PDFMinerPDFasHTMLLoader\\n- PDFPlumberLoader\\n- PagedPDFSplitter\\n- PlaywrightURLLoader\\n- PolarsDataFrameLoader\\n- GitHubIssuesLoader\\n- GitLoader\\n- GitbookLoader\\n- GoogleApiClient\\n- GoogleApiYoutubeLoader\\n- GoogleDriveLoader\\n- GutenbergLoader\\n- HNLoader\\n- HuggingFaceDatasetLoader\\n- IFixitLoader\\n- IMSDbLoader\\n- ImageCaptionLoader\\n- IuguLoader\\n- JSONLoader\\n- JoplinLoader\\n- LarkSuiteDocLoader\\n- MHTMLLoader\\n- MWDumpLoader\\n- MastodonTootsLoader\\n- MathpixPDFLoader\\n- MaxComputeLoader\\n- MergedDataLoader\\n\\nこれらは一部のモジュールの例であり、すべてのDocument Loaderモジュールを網羅しているわけではありません。'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa (\"vector storeをファイルに保存して何回もEmbedding　APIに問い合わせないようにしたいので実装を教えてください\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF9gnVIjuwRH",
        "outputId": "eb5aa413-8a01-4e37-be6b-39ca1242718f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'vector storeをファイルに保存して何回もEmbedding\\u3000APIに問い合わせないようにしたいので実装を教えてください',\n",
              " 'chat_history': [SystemMessage(content=\"\\nThe human asked for a detailed explanation of LangChain. The AI explained that LangChain is a Python package for performing natural language processing tasks, containing multiple chains for performing tasks such as preprocessing text data, feature engineering, training models, and inference. Version 2.0.0 of LangChain is currently available, but version 3.0.0 is scheduled to be discontinued. The AI recommended using the latest version of LangChain and mentioned that it can perform various tasks such as providing basic information, explaining detailed information and concepts, participating in conversations and discussions, providing help and advice, and automating tasks. The AI also explained that LangChain can be used to retrieve the latest Google search data for the desired information, by using the Google search API to get search results related to the specified keyword, and then using the results as input for LangChain to process and extract the necessary information. However, the exact implementation of LangChain varies depending on the project, so the AI cannot provide detailed steps. The AI suggested to implement the appropriate LangChain according to the project's requirements and objectives, and recommended referring to the LangSmith project's documentation and source code for more detailed steps. When asked about specific modules that could increase the productivity of software engineers, the AI mentioned some examples\", additional_kwargs={})],\n",
              " 'answer': 'LangChainを使用してvector storeをファイルに保存し、Embedding APIへの問い合わせを最小限に抑える方法は次のとおりです。\\n\\n1. `langchain.vectorstores`モジュールから必要なクラスをインポートします。たとえば、`ElasticsearchStore`を使用する場合は次のようにします。\\n\\n```python\\nfrom langchain.vectorstores import ElasticsearchStore\\n```\\n\\n2. 使用する埋め込みモデル（例：OpenAIEmbeddings）をインポートします。以下のようになります。\\n\\n```python\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\n```\\n\\n3. `ElasticsearchStore`のインスタンスを作成します。以下の例では、`index_name`を`langchain-demo`、ElasticsearchのURLを`http://localhost:9200`としています。\\n\\n```python\\nvectorstore = ElasticsearchStore(\\n    embedding=OpenAIEmbeddings(),\\n    index_name=\"langchain-demo\",\\n    es_url=\"http://localhost:9200\"\\n)\\n```\\n\\n4. vector storeをファイルに保存します。`vectorstore.save()`メソッドを使用します。以下の例では、`vectorstore.bin`という名前のファイルに保存しています。\\n\\n```python\\nvectorstore.save(\"vectorstore.bin\")\\n```\\n\\n5. vector storeをファイルから読み込むには、`vectorstore.load()`メソッドを使用します。以下の例では、`vectorstore.bin`という名前のファイルから読み込んでいます。\\n\\n```python\\nvectorstore.load(\"vectorstore.bin\")\\n```\\n\\n6. vector storeを使用してクエリを実行するには、`vectorstore.query()`メソッドを使用します。以下の例では、クエリを`query_text`という変数に格納し、`vectorstore.query()`メソッドを呼び出しています。\\n\\n```python\\nquery_text = \"example query\"\\nresults = vectorstore.query(query_text)\\n```\\n\\nこれらの手順に従うことで、vector storeをファイルに保存し、Embedding APIへの問い合わせを最小限に抑えることができます。'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "if os.path.exists(repo_path):\n",
        "    shutil.rmtree(repo_path)\n",
        "    print(f\"{repo_path} を削除しました。\")\n",
        "else:\n",
        "    print(f\"{repo_path} は存在しません。\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6whfj3z3OnB",
        "outputId": "580cdb94-b866-48fe-c463-7511081a117d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./tempo/ を削除しました。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yD-zQjJnEQ9G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}